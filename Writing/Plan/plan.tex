% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
% \geometry{margins=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent
%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...

\usepackage{natbib}
\usepackage{shortbold}

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% END Article customizations

%%% The "real" document content comes below...

\title{Fast low-rank metric learning}
\author{Dan Onea\c{t}\u{a}}
\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}
\maketitle

\section{Introduction}
	\begin{itemize}
		\item	Motivation.
		\item 	Thesis structure. Briefly describe how the dissertation is organized.
		\item 	Explain the mathematical notation used through-out the thesis.
	\end{itemize}

\section{Background}
	\subsection{Theoretical background}
		\begin{itemize}
			\item Metrics. What is a distance metric? Properties.
			\item Mahalanobis-like metric. What are its advantages? Present equivalence between learning a Mahalanobis-like metric and learning a linear projection or doing feature extraction.
			
		\end{itemize}
		
	\subsection{Related methods}
		\begin{itemize}
			\item Introduce some of the most representative methods for metric learning: Mahalanobis metric for clustering \citep{xing2003}, relevant component analysis \citep{shental2002}.
			\item Present methods inspired by NCA: metric learning by class collapsing \citep{globerson2006}, non-linear NCA \citep{salakhutdinov2007}, largest margin nearest neighbour \citep{weinberger2009}.
			\item Present work done for fast metric learning \citep{weinberger2007, weinberger2009}.
			\item Refer to some papers where NCA was used for practical applications: \textit{e.g.}, \citep{keller2006, singh2010}.
		\end{itemize}
		
\section{Neighbourhood component analysis}
	\subsection{General presentation}
		\begin{itemize}
			\item Describe and interpret NCA equations.
			\item Advantages: improves accuracy, useful for dimensionality reduction, assumption-free.
			\item Drawbacks: non-convexity, gradient evaluation is expensive $\mathcal{O}(N^2D^2)$, how to classify a given point (use $k$NN or NCA objective function).
		\end{itemize}
	\subsection{Practical issues}
		\begin{itemize}
			\item Objective function is not convex: how can we avoid local optima? Try different initializations (random, PCA, LDA, RCA). Use different optimization methods (gradient descent, conjugate gradients). Try annealing the dimensionality gradually.
		\end{itemize}
	\subsection{NCA as KDE}
		\begin{itemize}
			\item Present NCA as a class-conditional kernel density estimation problem. 
		\end{itemize}
		
\section{Reducing the computational cost}
	\subsection{Sub-sampling}
		\begin{itemize}
			\item Na\"ive idea. Easy to implement: use a random subset $\mathcal{D}_n\subseteq \mathcal{D}$ to learn the projection matrix $\AB$.
			\item For classification we can use the entire data set.
			\item Reduces the computational cost of the gradient to $\mathcal{O}(n^2D^2)$.
			\item Drawbacks: does not use the whole information available. Also the final result is affected by the thinner distribution of the points that results by randomly sub-sampling points.
		\end{itemize}
	\subsection{Mini-batches}
		\begin{itemize}
			\item Again this is motivated by the fact that the gradient complexity is $\mathcal{O}(n^2D^2)$. This time we use the entire data set: we iteratively use different subsets from $\mathcal{D}$.
			\item Different possibilities:
				\begin{enumerate}
					\item Randomly selected mini-batches. These need to be quite large to ensure convergence.
					\item Batches selected by clustering: farthest point clustering, recursive projection clustering, agglomerative clustering using $k$-d trees.
				\end{enumerate}
		\end{itemize}
	\subsection{Stochastic learning}
	\label{subsec:stochastic-learning}
		\begin{itemize}
			\item Instead of updating the projection matrix $\AB$ after one sweep of the entire data set, we can update it gradually after seeing fewer points: we need only a general direction in the parameter space.
			\item This can be used for on-line learning.
		\end{itemize}
	\subsection{Approximate computations}
		\begin{itemize}
			\item Motivated by the fact that only a few contributions are significant. Brute pruning (using only points for whose contribution is greater than a certain threshold $p_{ij}>\epsilon$ + using only the first $k$ neighbours for each point) can be useful.
			\item In a more principled manner, we can use the KDE interpretation and  $k$-d trees.
			\item This can be further accelerated by training in a stochastic fashion as described previously (subsection \ref{subsec:stochastic-learning}).
		\end{itemize}
	\subsection{Exact computations}
		\begin{itemize}
			\item In the model, we can replace the squared exponential kernel with a compact support kernel. In this manner we will have exact computations. We could use $k$-d trees to quickly do range searches.
			\item We can use the simplest polynomial kernel that satisfies differentiability, although other kernels are probably very similar.
			\item Must take care to start with the points positioned in such a way that each point has at least one neighbour within its range. 
			\item Again we can use the idea from subsection \ref{subsec:stochastic-learning}.
		\end{itemize}
	\subsection{NCA with compact support kernels and background distribution}
		\begin{itemize}
			\item An extended case that takes care of the scenario when a point remains unallocated by using a Gaussian background distribution for each class. This can be interpreted in a generative model. 
		\end{itemize}
		
\section{Evaluation}

\section{Conclusions}

\bibliographystyle{apalike}
%% Specify the bibliography file. Default is thesis.bib.
\bibliography{thesis}

\section*{Appendix}
	This section will consists of further results (tables, figures, graphs) and mathematical derivations that were not included in the main text.

\end{document}
