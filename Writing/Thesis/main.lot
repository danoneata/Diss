\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {5.1}{\ignorespaces \relax \fontsize {10.95}{13.6}\selectfont \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \relax \fontsize {10.95}{13.6}\selectfont \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This table presents the characteristics of the data sets used: number of samples $N$, dimensionality of the data $D$ and number of classes $C$. The two digits data sets \texttt {mnist} and \texttt {usps} were downloaded from the following URL \url {http://cs.nyu.edu/\nobreakspace {}roweis/data.html}. All the others data sets are available in the UCI repository \url {http://archive.ics.uci.edu/ml/datasets.html}.}}{44}
\contentsline {table}{\numberline {5.2}{\ignorespaces \relax \fontsize {10.95}{13.6}\selectfont \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Accuracy of standard NCA on four small data sets. Scores are averaged over 40 runs. The second column presents the dimensionality\nobreakspace {}$d$ the data set is reduced to. The last column shows the leave one out cross validation performance on the data set using Euclidean metric.}}{46}
\contentsline {table}{\numberline {5.3}{\ignorespaces \relax \fontsize {10.95}{13.6}\selectfont \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Accuracy of three linear transformation techniques applied on the large data sets. We used $1$-NN for classification. Scores are averaged over 20 runs, except for \texttt {mnist} data set. We reduced each data set dimensionality to $d=5$.}}{46}
\contentsline {table}{\numberline {5.4}{\ignorespaces Accuracy scores for SS method on the larger data sets. We used RCA for initialization and CGs for optimization. We used a subset of $n=3000$ data points for training and the whole data set for testing.}}{48}
\contentsline {table}{\numberline {5.5}{\ignorespaces Accuracy scores for MB method on the larger data sets. We used RCA for initialization and the mini-batches were clustered in the low-dimensional space using RPC. The size of a mini-batch was of maximum $n=2000$ data points.}}{49}
\contentsline {table}{\numberline {5.6}{\ignorespaces Accuracy scores for SL method on the small data sets. We used RCA for initialization. The scores are averaged after 20 iterations.}}{50}
\contentsline {table}{\numberline {5.7}{\ignorespaces Accuracy scores for SS method on the larger data sets. We used RCA for initialization. At each iteration we consider $n=50$ data points.}}{50}
\contentsline {table}{\numberline {5.8}{\ignorespaces NCA SL + $k$-d trees on \texttt {landsat}. $\epsilon _{\qopname \relax m{max}}$ denotes the maximum error that we accept while approximating the density for a point given a class\nobreakspace {}$p(\mathbf {x}|c)$. Visited points indicates the fraction of points that are used for computing the function and the gradient.}}{54}
\contentsline {table}{\numberline {5.9}{\ignorespaces NCA SL + $k$-d trees on large data sets. For these experiments we set $\epsilon _{\qopname \relax m{max}}=0.1$.}}{54}
\contentsline {table}{\numberline {5.10}{\ignorespaces Accuracy scores for NCA SL CS method on the larger data sets.}}{55}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {B.1}{\ignorespaces Accuracy scores of NCA trained using stochastic learning on small and medium sized data sets. The scores are averaged over 20 repeats. RCA was used for initialization.}}{65}
\contentsline {table}{\numberline {B.2}{\ignorespaces Accuracy scores of NCA trained using stochastic learning on small and medium sized data sets. The scores are averaged over 20 repeats. RCA was used for initialization.}}{66}
\contentsline {table}{\numberline {B.3}{\ignorespaces Comparison in terms of accuracy between two possible optimization methods for NCA: conjugate gradients and gradient ascent with the ``bold driver'' heuristic}}{67}
\contentsline {table}{\numberline {B.4}{\ignorespaces Comparison in terms of accuracy between two possible optimization methods for NCA: conjugate gradients and gradient ascent with the ``bold driver'' heuristic}}{68}
\contentsline {table}{\numberline {B.5}{\ignorespaces Accuracy scores for the compact support versions of NCA.}}{69}
\contentsline {table}{\numberline {B.6}{\ignorespaces Accuracy scores for the compact support versions of NCA.}}{70}
