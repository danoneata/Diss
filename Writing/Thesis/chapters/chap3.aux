\relax 
\citation{goldberger2004}
\citation{goldberger2004}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Neighbourhood component analysis}{9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:nca}{{3}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}General presentation}{9}}
\newlabel{sec:general-presentation}{{3.1}{9}}
\newlabel{eq:stochastic-neighbour}{{3.1}{10}}
\newlabel{eq:nca-p-i}{{3.2}{10}}
\newlabel{eq:nca-obj}{{3.3}{10}}
\newlabel{eq:nca-grad}{{3.4}{11}}
\citation{singh2010}
\citation{rall1981}
\citation{weinberger2007}
\citation{weizman2007}
\newlabel{eq:nca-grad-alt}{{3.5}{12}}
\citation{weinberger2007}
\citation{singh2010}
\citation{barber2011}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces NCA as a class-conditional kernel density estimation model. The figure represents a schematic illustration of a mixture of Gaussians. The red circles denote isotropic Gaussians. These are centred onto the points that belong to a certain class. A point\nobreakspace  {}$i$ is generated by either of the components of the class with a probability inverse proportional with the distance.}}{13}}
\newlabel{fig:kde}{{3.1}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Class-conditional kernel density estimation interpretation}{13}}
\newlabel{sec:cc-kde}{{3.2}{13}}
\newlabel{eq:p-x-c}{{3.9}{14}}
\newlabel{eq:nca-cc-kde-bayes}{{3.11}{14}}
\newlabel{eq:nca-cc-kde-obj}{{3.13}{14}}
\citation{bishop1995}
\newlabel{eq:nca-cc-kde-grad}{{3.14}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Practical notes}{15}}
\newlabel{sec:practical-notes}{{3.3}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Optimization methods}{15}}
\newlabel{subsec:optimization}{{3.3.1}{15}}
\citation{vogl1988}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3.1}{\ignorespaces Gradient ascent (batch version)}}{16}}
\newlabel{alg:gd}{{3.1}{16}}
\newlabel{alg:gd-init}{{1}{16}}
\citation{lecun1998}
\citation{bishop1995}
\citation{shewchuk1994}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces This figure illustrates the early stopping procedure. During training we keep a small part of the data set apart, for cross-validation. On this cross-validation data set we compute the error function at each iteration. When the error function stops decreasing we stop the learning procedure and return to the parameter that achieved the best value. This figure resulted while applying NCA on the \texttt  {mnist} data set. The best value was obtained at iteration 98, as indicated by the red dot.}}{18}}
\newlabel{fig:early-stopping}{{3.2}{18}}
\citation{pearson1901}
\citation{fisher1936}
\citation{bar2003}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Initialization}{19}}
\newlabel{subsec:initialization}{{3.3.2}{19}}
\newlabel{eq:pca-1}{{3.15}{19}}
\newlabel{eq:lda-1}{{3.16}{19}}
\newlabel{eq:lda-2}{{3.17}{19}}
\newlabel{fig:init-1}{{3.3(a)}{20}}
\newlabel{sub@fig:init-1}{{(a)}{20}}
\newlabel{fig:init-2}{{3.3(b)}{20}}
\newlabel{sub@fig:init-2}{{(b)}{20}}
\newlabel{fig:init-3}{{3.3(c)}{20}}
\newlabel{sub@fig:init-3}{{(c)}{20}}
\newlabel{fig:init-4}{{3.3(d)}{20}}
\newlabel{sub@fig:init-4}{{(d)}{20}}
\newlabel{fig:init-7}{{3.3(e)}{20}}
\newlabel{sub@fig:init-7}{{(e)}{20}}
\newlabel{fig:init-8}{{3.3(f)}{20}}
\newlabel{sub@fig:init-8}{{(f)}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Results for different initializations (random, PCA and LDA) on \texttt  {wine} data set. The images on the left side represent the projections of the data set using the initial $\mathbf  {A}$. The images on the right side are data set projection with the final $\mathbf  {A}$. Above each figure there is presented the LOOCV score which is normalized to 100.}}{20}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Initial random projection}}}{20}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {NCA projection after random initialization}}}{20}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Initial projection using PCA}}}{20}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {NCA projection after PCA initialization}}}{20}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {Initial projection using LDA}}}{20}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {NCA projection after LDA initialization}}}{20}}
\newlabel{fig:init}{{3.3}{20}}
\citation{butman2008}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Numerical issues}{21}}
\newlabel{subsec:numerical-issues}{{3.3.3}{21}}
\citation{goldberger2004}
\citation{butman2008}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Regularization}{22}}
\newlabel{subsec:regularization}{{3.3.4}{22}}
\citation{singh2010}
\citation{goldberger2004}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}Doing classification}{23}}
\newlabel{subsec:doing-classification}{{3.3.5}{23}}
\newlabel{eq:nca-cls}{{3.25}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.6}Dimensionality annealing}{23}}
\newlabel{subsec:dimensionality-annealing}{{3.3.6}{23}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3.2}{\ignorespaces Dimensionality annealing}}{24}}
\newlabel{alg:dimensionality-annealing}{{3.2}{24}}
\newlabel{alg:dim-anneal-select-dim}{{2}{24}}
\newlabel{fig:sa-1}{{3.4(a)}{25}}
\newlabel{sub@fig:sa-1}{{(a)}{25}}
\newlabel{fig:sa-2}{{3.4(b)}{25}}
\newlabel{sub@fig:sa-2}{{(b)}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Example of applying the dimensionality annealing procedure to \texttt  {ionosphere} data set. The method using dimensionality annealing is more visually appealing than the classical approach.}}{25}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Simple NCA transformation}}}{25}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {NCA with dimensionality annealing projection}}}{25}}
\newlabel{fig:simulated-annealing}{{3.4}{25}}
\@setckpt{chapters/chap3}{
\setcounter{page}{26}
\setcounter{equation}{27}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{2}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{3}
\setcounter{subsection}{6}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{4}
\setcounter{table}{0}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{16}
\setcounter{algorithm}{2}
\setcounter{ALC@line}{7}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{subfigure}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{lotdepth}{1}
\setcounter{lstlisting}{0}
\setcounter{lstnumber}{1}
}
