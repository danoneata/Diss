\relax 
\citation{singh2010}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Reducing the computational cost}{22}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:reducing}{{4}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Sub-sampling}{22}}
\newlabel{sec:sub-sampling}{{4.1}{22}}
\newlabel{fig:sub-sampling-1}{{4.1(a)}{23}}
\newlabel{sub@fig:sub-sampling-1}{{(a)}{23}}
\newlabel{fig:sub-sampling-2}{{4.1(b)}{23}}
\newlabel{sub@fig:sub-sampling-2}{{(b)}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Result of sub-sampling method on \texttt  {wine}. There were used one third of the original data set for training, \textit  {i.e.}, $n = N/3$. We note that the points that belong to the sub-set $\mathcal  {D}_n$ are perfectly separated. But after applying the metric to the whole data there appear different misclassification errors. The effects are even more acute if we use smaller sub-sets.\relax }}{23}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Learnt projection $\mathbf {A}$ on the sub-sampled data set $\mathcal {D}_n$.}}}{23}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {The projection $\mathbf {A}$ applied to the whole data set $\mathcal {D}$.}}}{23}}
\newlabel{fig:sub-sampling}{{4.1}{23}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Mini-batches}{23}}
\newlabel{sec:mini-batches}{{4.2}{23}}
\citation{gonzalez1985}
\citation{chalupka2011}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.1}{\ignorespaces Training algorithm using mini-batches formed by clustering\relax }}{24}}
\newlabel{alg:mini-batches}{{4.1}{24}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.2}{\ignorespaces Farthest point clustering\relax }}{24}}
\newlabel{alg:fpc}{{4.2}{24}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.3}{\ignorespaces Recursive projection clustering\relax }}{25}}
\newlabel{alg:rpc}{{4.3}{25}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Stochastic learning}{25}}
\newlabel{sec:stochastic-learning}{{4.3}{25}}
\citation{goldberger2004}
\citation{weinberger2007,singh2010}
\newlabel{eq:snca-grad}{{4.2}{26}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.4}{\ignorespaces Stochastic learning for NCA (sNCA)\relax }}{26}}
\newlabel{alg:sNCA}{{4.4}{26}}
\citation{weinberger2007}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Evolution of the stochastic assignments $p_{ij}$ during training for a given point $\mathbf  {x}_i$.\relax }}{27}}
\newlabel{fig:contributions}{{4.2}{27}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Approximate computations}{27}}
\newlabel{sec:approximate}{{4.4}{27}}
\citation{deng1995,gray2003}
\citation{bentley1975}
\citation{friedman1977}
\citation{moore1991}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}$k$-d trees}{28}}
\newlabel{subsec:k-d-trees}{{4.4.1}{28}}
\newlabel{fig:kdtree-1}{{4.3(a)}{29}}
\newlabel{sub@fig:kdtree-1}{{(a)}{29}}
\newlabel{fig:kdtree-2}{{4.3(b)}{29}}
\newlabel{sub@fig:kdtree-2}{{(b)}{29}}
\newlabel{fig:kdtree-3}{{4.3(c)}{29}}
\newlabel{sub@fig:kdtree-3}{{(c)}{29}}
\newlabel{fig:kdtree-4}{{4.3(d)}{29}}
\newlabel{sub@fig:kdtree-4}{{(d)}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Illustration of the $k$-d tree with bounding boxes at different levels of depths. This figure also outlines the building phases of the tree.\relax }}{29}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Root node}}}{29}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {First level}}}{29}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Second level}}}{29}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Last level}}}{29}}
\newlabel{fig:kdtree}{{4.3}{29}}
\citation{lang2009}
\citation{goldberger2004}
\newlabel{eq:splitting-direction}{{4.4}{30}}
\newlabel{eq:subset-left}{{4.5}{30}}
\newlabel{eq:subset-right}{{4.6}{30}}
\citation{gray2001,gray2003,gray2003b}
\citation{shen2006}
\citation{gray2001}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.5}{\ignorespaces $k$-d tree building algorithm\relax }}{31}}
\newlabel{alg:kd-tree-build}{{4.5}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Approximate kernel density estimation}{31}}
\newlabel{subsec:approx-kde}{{4.4.2}{31}}
\citation{moore1991}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Approximate KDE for NCA}{33}}
\newlabel{subsec:approx-KDE-for-NCA}{{4.4.3}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Exact computations}{33}}
\newlabel{sec:exact-computations}{{4.5}{33}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.6}{\ignorespaces Approximate NCA objective function and gradient computation\relax }}{34}}
\newlabel{alg:cc-kde-nca}{{4.6}{34}}
\newlabel{eq:cs-1}{{4.11}{34}}
\newlabel{eq:stochastic-neighbours-cs}{{4.13}{35}}
\newlabel{eq:nca-cs-grad-kernel}{{4.15}{35}}
\newlabel{eq:nca-cs-grad}{{4.16}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}NCA with compact support kernels and background distribution}{35}}
\newlabel{sec:nca-cs-back}{{4.6}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Neighbourhood component analysis with compact support kernels and background distribution. The main assumption is that each class is a mixture of compact support distributions $k(\mathbf  {x}|\mathbf  {x}_j)$ plus a normal background distribution $\mathcal  {N}(\mathbf  {x}|\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$\mu $},\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$\Sigma $})$.\relax }}{36}}
\newlabel{fig:cs-back}{{4.4}{36}}
\newlabel{eq:nca-cs-back-1}{{4.18}{36}}
\@setckpt{chapters/chap4}{
\setcounter{page}{38}
\setcounter{equation}{20}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{4}
\setcounter{section}{6}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{4}
\setcounter{table}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{16}
\setcounter{algorithm}{6}
\setcounter{ALC@unique}{66}
\setcounter{ALC@line}{11}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{subfigure}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{lotdepth}{1}
\setcounter{lstnumber}{1}
\setcounter{lstlisting}{0}
}
