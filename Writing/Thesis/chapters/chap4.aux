\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Reducing the computational cost}{4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:reducing}{{2}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Sub-sampling}{4}}
\newlabel{sec:sub-sampling}{{2.1}{4}}
\citation{gonzalez1985}
\citation{chalupka2011}
\newlabel{fig:sub-sampling-1}{{2.1(a)}{5}}
\newlabel{sub@fig:sub-sampling-1}{{(a)}{5}}
\newlabel{fig:sub-sampling-2}{{2.1(b)}{5}}
\newlabel{sub@fig:sub-sampling-2}{{(b)}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Result of sub-sampling method on \texttt  {wine}. There were used one third of the original data set for training, \textit  {i.e.}, $n = N/3$. We note that the points that belong to the sub-set $\mathcal  {D}_n$ are perfectly separated. But after applying the metric to the whole data there appear different misclassifications. The effects are even more acute if we use smaller sub-sets.\relax }}{5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Learnt projection $\mathbf {A}$ on the sub-sampled data set $\mathcal {D}_n$.}}}{5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {The projection $\mathbf {A}$ applied to the whole data set $\mathcal {D}$.}}}{5}}
\newlabel{fig:sub-sampling}{{2.1}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Mini-batches}{5}}
\newlabel{sec:mini-batches}{{2.2}{5}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.1}{\ignorespaces Training algorithm using mini-batches formed by clustering\relax }}{6}}
\newlabel{alg:mini-batches}{{2.1}{6}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.2}{\ignorespaces Farthest point clustering\relax }}{6}}
\newlabel{alg:fpc}{{2.2}{6}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.3}{\ignorespaces Recursive projection clustering\relax }}{7}}
\newlabel{alg:rpc}{{2.3}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Stochastic learning}{8}}
\newlabel{sec:stochastic-learning}{{2.3}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Approximate computations}{8}}
\newlabel{sec:approximate}{{2.4}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Exact computations}{8}}
\newlabel{sec:exact-computations}{{2.5}{8}}
\newlabel{eq:cs-1}{{2.1}{8}}
\newlabel{eq:cs-2}{{2.2}{8}}
\newlabel{eq:nca-cs-grad}{{2.7}{9}}
\@setckpt{chapters/chap4}{
\setcounter{page}{10}
\setcounter{equation}{7}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{5}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{1}
\setcounter{table}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{3}
\setcounter{ALC@unique}{23}
\setcounter{ALC@line}{9}
\setcounter{ALC@rem}{9}
\setcounter{ALC@depth}{1}
\setcounter{subfigure}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{lotdepth}{1}
}
