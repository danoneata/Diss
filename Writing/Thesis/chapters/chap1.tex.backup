\chapter{Introduction}
\label{ch:introduction}

$k$ nearest neighbours ($k$NN) is one of the oldest and simplest classification methods. It has its origins in an unpublished technical report by \citet{fix1951} and since then it became standard textbook material \citep{russell1996,mitchell1997,bishop2006}. In the last 50 years, $k$NN was present in most of the machine learning related fields (pattern recognition, statistical classification, data mining, information retrieval, data compression) and it plays a role in many applications (e.g., face recognition, plagiarism detection, vector quantization).

The idea behind $k$NN is intuitive and straightforward: classify a given point according to a majority vote of its neighbours; the selected class is the one that is the most represented amongst the $k$ nearest neighbours. This is easy to implement and it usually represents a good way to approach new problems or data sets. Despite its simplicity $k$NN is still a powerful tool, performing surprisingly well in practice \citep{holte1993}.

Yet there are also other characteristics that make $k$NN an interesting method. First of all, $k$NN makes no assumptions about the underlying structure of the data. No a priori knowledge is needed beforehand, but we let the data ``speak for itself''. The accuracy increases with the number of points in the data set and, in fact, it approaches Bayes optimality as the cardinality of the training set goes to infinity and $k$ is sufficiently large \citep{cover1967}. Secondly, $k$NN is able to represent complex functions with non-linear decision boundaries by using only simple local approximations
% (this behaviour is hardly captured by any parametric method). 
Lastly, $k$NN operates in a ``lazy'' fashion. The training data set is just stored and its use is delayed until testing. The quasi-inexistent training
%\footnote{There can be a training step in which the parameter $k$ is tuned via cross-validation or we can we imagine different technique being applied for dimensionality reduction}
 allows to easily add new training examples. 

$k$NN has some drawbacks that influence both the computational performance and the quality of its predictions. Since $k$NN has to store all the exemplars, the memory requirements are directly proportional with the number of instances in the training set. The cardinality of the data also influences the method's speed. All computations are done at testing time, making $k$NN painfully slow when applied on large data sets. 
%The cost of classification is also linear in the cardinality of the training set because all the computations are done at testing. and it is often prohibitive for large data sets. 

The accuracy of $k$NN is closely related to how we define what ``close'' and ``far'' mean for our data set and task. Mathematically, the notion of dissimilarity is incorporated into $k$NN by using different distance metrics. Usually the standard Euclidean metric is not satisfactory and the aim is to find that particular metric that gives the best results on our data set for a given task (section~\ref{sec:theoretical-background}).
%However, more important are the issues related to the accuracy. On one hand, there is not clear how we should choose a dissimilarity metric and how notions as ``close''/``far'' are defined for our data set. This is non-trivial and usually the standard Euclidean metric is not satisfactory (see Subsection \ref{subsec:distance-metrics}, for a more detailed discussion). On the other hand, there  
%All these problems are even more acute nowadays when we have to operate on huge sets of data with many attributes (e.g., images, videos, DNA sequences, etc.). 
There is an entire literature that tries to come up with possible solutions (section~\ref{sec:related-methods}).
 
This thesis focuses on neighbourhood component analysis (NCA; \citealp{goldberger2004}). NCA method learns the metric that maximizes the expected number of correctly classified points (section~\ref{sec:general-presentation}). Using the NCA metric with $k$NN usually improves the performance over simple $k$NN, since we use the label information to construct a suitable metric that selects the relevant attributes. If we restrict the metric to be low ranked we can find its associated linear projection. A low dimensional representation of the original data reduces the storage needs and the computational expense at test time . Also it alleviates some of the concerns that have been raised regarding the usefulness of nearest neighbours methods for high dimensional data \citep{beyer1999, hinneburg2000}. The curse of dimensionality arises for $k$NN, because the distances become indiscernible for many dimensions. For a given distribution the maximum and the minimum distance between points become equal in the limit of dimensions. 
NCA proves to be an elegant answer for the above issues and, consequently, it was successfully used in a variety of applications: face recognition \citep{butman2008}, hyper-spectral image classification \citep{weizman2007}, acoustic modelling \citep{singh2010} and even reinformcent learning \citep{keller2006}. 

However, the method introduces a consistent training time. The objective function needs the pairwise distances of the points, so the function evaluation is quadratic in the number of data points. Also the optimization process is iterative. For large data sets, NCA training becomes prohibitively slow. For example, \citet{weinberger2009} reported that the original implementation ran out of RAM and \citet{weizman2007} had to use only 10\% of the data set in order to successfully train the model. There is only little previous work that uses NCA for large scaled applications. One example is \citep{singh2010}, who parallelizes the computations across multiple computers and adopts various heuristics to prune terms of the objective function and the gradient.
%One elegant answer is provided by \citet{goldberger2004}. They proposed a new method, Neighbourhood Component Analysis (NCA), that copes with the drawbacks in a unified manner by learning a low-rank metric. This reduces both the storage and the computational cost, because the algorithm uses the data set projected into a lower subspace, with fewer attributes needed. Also the accuracy is improved, because the label information is used for constructing a proper metric that selects the relevant attributes. However, NCA introduces a consistent training time, because we now have an objective function whose gradient is quadratic in the number of data points that is optimized using iterative methods. 

The main aim of this project is to reduce NCA training time without significant loses in accuracy (chapter \ref{ch:reducing}). We start our investigation with the most simple ideas: use only a subset of the data set (section \ref{sec:sub-sampling}) or train the metric on different mini-batches subsampled from the data set (section \ref{sec:mini-batches}). This last idea can be further refined by using clustered mini-batches. Also we present an alternative mini-batch algorithm (section \ref{sec:stochastic-learning}) that decreases the theoretical cost. 

These methods can achieve further speedings if we use approximations of the objective function. Simple approximation ideas (such as ignoring the points which are farther away than a certain distance from the current point) were mentioned in the original paper \citep{goldberger2004} and they are re-iterated by \citet{weinberger2007} and \citet{singh2010}. We present a more principled approach that borrows ideas from fast kernel density estimation problems \citep{deng1995,gray2003,gray2003b}. We first recast NCA into a class-conditional kernel density estimation framework (section \ref{sec:cc-kde}) and next we present how fast density estimation is done using a space partitioning structure, such as $k$-d trees (section \ref{sec:approximate}). Similar work of fast metric learning is \citep{weinberger2008} and it uses ball trees \citep{omohundro1989} to accelerate a different distance metric technique, large margin nearest neighbour (LMNN; \citealp{weinberger2009}). However, LMNN is different from NCA and the way in which ball trees are applied also differs from our approach.
%$k$-d trees \citep{bentley1975} group together near points and can be used to quickly select only those points that have significant contribution. These sort of  have been successfully applied for speeding up $k$NN at query time \citep{friedman1977} and we will use them in a similar manner for NCA's testing operations. 

An alternative for the approximation method is to change the model such that it allows exact and efficient computations. In the kernel density estimation model, we can use compact support kernels instead of the standard Gaussian functions. The expense is reduced because only the points that lie in the compact support are used for estimating the density (section \ref{sec:exact-computations}).



%, we could interpret NCA as a class-conditional kernel-density estimate and then use different types of kernels instead of the Gaussian ones; we will focus especially on kernels with compact support, because they do not introduce approximation errors and computations can be done more efficiently than in the previous case. 
