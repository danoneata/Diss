\chapter{Conclusions}
\label{ch:conclusions}

This thesis presented our approach to fast low-rank metric learning. The need for a low rank metric was motivated in the context of $k$NN (chapter~\ref{ch:introduction} and section~\ref{sec:theoretical-background}), but we argue that learning a metric is useful whenever our algorithm relies on dissimilarities. Our efforts were directed towards the already established method, neighbourhood component analysis (NCA; section~\ref{sec:general-presentation}). We introduced a family of algorithms to mitigate NCA's main drawback --- the computational cost. In our attempt of speeding up NCA, we encountered other interesting theoretical and practical challenges. The answers to these issues represent an important part of the thesis (sections~\ref{sec:cc-kde} and~\ref{sec:practical-notes}). We summarize here our main contributions and present the conclusions.
\begin{description}
  \item[Section~\ref{sec:cc-kde}] The class-conditional kernel density estimation (CC-KDE) interpretation offers flexibility to NCA and opens a new range of methods. We can easily change the model in a suitable way by redefining the conditional probability~$p(\xB|c)$. Other advantages are (i) the possibility of including prior class distribution~$p(c)$ into the model and (ii) we can use the posterior probability~$p(c|\xB)$ for classification or in other scenarios that arise in decision theory.
  % for problems with very unbalanced classes and if we want to integrate decision making aspects.
  \item[Section~\ref{sec:practical-notes}] The practical advice in this section is helpful for those who are interested in applying the classic version of NCA\@. The ideas are particularly useful because we are optimizing a non-convex function. We believe that using relevant component analysis (RCA; \citealp{bar2003}) for initialization and conjugate gradients for optimization works best on small and medium-sized data sets. This combination does not require tuning any parameters so it can be readily applied on any data set. Among others, we highlight the dimensionality annealing technique; this look promising: it obtained good projections that seem to avoid local optima.
  \item[Section~\ref{sec:mini-batches}] We gave several ideas of adapting mini-batch (MB) techniques in a suitable way for NCA method. We tested a version based on recursive projection clustering (RPC, \citealp{chalupka2011}). The size of each mini-batch still has to be quite large to get significant gradients (we used $n=2000$). For large data set this already represents a significant improvement in terms of cost, but we question whether there is a more principled method of selecting $n$, the size of a cluster.
  \item[Section~\ref{sec:stochastic-learning}] We presented a stochastic learning (SL) algorithm that is theoretically motivated by stochastic optimization arguments. Empirical investigation demonstrated that this method achieves very close scores to the classic NCA\@. The SL method scales with the number of the points~$N$, but it can be further accelerated using technique presented in sections~\ref{sec:approximate} and~\ref{sec:exact-computations}. A further advantage of SL is that can be used for online learning.
  \item[Section~\ref{sec:approximate}] Using the CC-KDE framework for NCA and inspired from previous work on fast KDE \citep{deng1995, gray2003}, we proposed an algorithm that evaluates efficiently an approximation of the NCA objective function and its corresponding gradient. This method is fastest when combined with SL method. Our experiments suggested that even if we accept a large maximum error~$\epsilon_{\max}=0.1$, we do not seem to lose in accuracy. We noticed a similar behaviour when we did brute pruning.
  \item[Section~\ref{sec:exact-computations}] Further alterations of NCA method are the compact support version (NCA CS) and the compact support and background distribution version (NCA CS BACK). NCA CS achieves considerable speed-ups because we do not have to compute the gradient terms for all the points. Also we observe that it convergences faster than the other methods. However its speed does come at a cost: the accuracy performance is slightly worse than for the rest of the methods. These results might suggest to try a different approach: use a heavy tailed distribution
\end{description}
 
%In chapter~\ref{ch:introduction} we motivated the need for metric learning in the general context of $k$NN, while section~\ref{sec:theoretical-background} advocated for metric learning using more exact mathematical arguments. We showed the equivalence between Mahalanobis metrics and linear transformation which we used for the rest of the thesis. Section~\ref{sec:related-methods} presented some of the related work in Mahalanobis metric learning.

%The focus of this thesis is the NCA method. Chapter~\ref{ch:nca} explores NCA in detail and presents our experience with the method. The theoretical aspect is reviewed in section~\ref{sec:general-presentation} and we also discuss the complexity issues in more detail than they were previously treated. Our main contributions start from section~\ref{sec:cc-kde} which shows a novel interpretation of NCA\@. The class-conditional kernel density estimation, offers certain advantages (like the possibility of integrating class distribution into the model) and opens a new range of methods. In section~\ref{sec:practical-notes} we give practical advice on applying NCA.  Initializing the optimization process with a cheap linear transformation such as LDA or RCA usually results in good final solutions. For the optimization technique there does not seem to be a generally valid answer. Using either regularization or early stopping in conjunction with a NCA-based classification function can boost further the performance. Dimensionality annealing has the premises of an interesting idea, yet we need to further experiment.

%Chapter~\ref{ch:reducing} investigates methods for reducing the computational cost. The classical mini-batches method is modified to work better in a low-rank metric scenario. To do this we used cheap ways of clustering the points in low dimensions and then applied NCA on successive clusters (section~\ref{sec:mini-batches}). We evaluated these methods in an empirical setting (chapter~\ref{ch:evaluation}) and 

\section{Further directions}
\label{sec:further-directions}

This projected involved many ideas. Unfortunately, the time was limited and we could only look at a part of the whole picture and we left some paths open. We aim to continue some of the work in the near future. 

We plan to further investigate the automatic differentiation procedure and see whether this is a better idea than symbolic differentiation. Automatic differentiation will help reducing the number of operations needed for the gradient computation, but it might imply a huge memory cost. This trade-off can make automatic differentiation impractical.

Implementing efficient $k$-d tree code for approximate CC-KDE algorithm should prove beneficial. It will allow our methods to run on even larger data sets making possible more comparisons between the methos.

We think that dimensionality annealing approach can be fruitful. However, this idea is still in its infant stages and more experimentation is needed to see whether it can be applied successfully on a variety of data sets. 

Further refinements of our work are still possible. It would be interesting to try a mini-batch and stochastic gradient combined algorithm: cluster points and compute mini-batch mini-batch contributions using a dual tree recursion \citep{gray2003}. 

% Ideas from this thesis can be reused for other distance metric learning methods, with different objective functions (e.g., \citealp{xing2003}). However, this implies losing the convexity.

We believe that other extensions of NCA are possible. For example, a low-dimensional feature extractor could be built using a two step NCA algorithm: first use a diagonal metric~$\SB$ to select the most relevant attributes, followed by a low-rank metric. 
