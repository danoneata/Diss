\chapter{Introduction}
\label{ch:introduction}

$k$ Nearest Neighbours ($k$NN) is one of the oldest and simplest classification methods. It has its origins in an unpublished technical report by \citet{fix1951} and since then it became standard textbook material \citep{russell1996,mitchell1997,bishop2006}. In the last 50 years, $k$NN was present in most of the machine learning related fields (pattern recognition, statistical classification, data mining, information retrieval, data compression) and it plays a role in many attractive applications (e.g., face recognition, plagiarism detection, vector quantization).

The idea behind $k$NN is intuitive and straightforward: classify a given point according to a majority vote of its neighbours; the selected class is the one that is the most represented amongst the $k$ nearest neighbours. This is easy to implement and it usually represents a good way to approach new problems or data sets. Despite its simplicity $k$NN is still a powerful tool, performing surprisingly well in practice, as most of the simple classifiers \citep{holte1993}.

Yet there are also other characteristics that make $k$NN an interesting method. First of all, $k$NN makes no assumptions about the underlying structure of the data, but lets the data ``speak for themselves''. This means that no a priori knowledge is needed beforehand and, more importantly, the accuracy increases with the number of points in the data set (in fact, it approaches Bayes optimality as the cardinality of the training set approaches infinity and $k$ is sufficiently large; \citealp{cover1967}). Secondly, $k$NN is able to represent complex functions with non-linear decision boundaries by using only simple local approximations (this behaviour is hardly captured by any parametric method). Lastly, $k$NN operates in a ``lazy'' fashion: the training data is just stored and their use is delayed until the testing. The quasi-inexistent training
%\footnote{There can be a training step in which the parameter $k$ is tuned via cross-validation or we can we imagine different technique being applied for dimensionality reduction}
 allows to easily add new training examples. 

On the other hand, $k$NN has some drawbacks that influence both the computational performance and also the quality of its predictions. Firstly, because it has to memorise all the exemplars, the storage requirements are directly proportional with the number of instances in the training set. Furthermore, a serious practical disadvantage is represented by the fact that all the computations are done at testing. The cost for this is also linear in the cardinality of the training set and it is often prohibitive for large data sets. However, more important are the issues related to the accuracy. On one hand, there is not clear how we should choose a dissimilarity metric and how notions as ``close''/``far'' are defined for our data set. This is non-trivial and usually the standard Euclidean metric is not satisfactory (see Subsection \ref{subsec:distance-metrics}, for a more detailed discussion). On the other hand, there have been raised concerns with the usefulness of Nearest Neighbours (NN) methods for high dimensional data \citep{beyer1999, hinneburg2000}. The curse of dimensionality arises for $k$NN, because the distances become indiscernible for many dimensions; alternatively formulated, for a given distribution the maximum and the minimum distance between points become equal in the limit of dimensions. 

All these problems are even more acute nowadays when we have to operate on huge sets of data with many attributes (e.g., images, videos, DNA sequences, etc.). There is an entire literature that tries to come up with possible solutions (some of the most prominent papers are discussed in Section \ref{sec:background}). One elegant answer is provided by \citet{goldberger2004}. They proposed a new method, Neighbourhood Component Analysis (NCA), that copes with the drawbacks in a unified manner by learning a low-rank metric. This reduces both the storage and the computational cost, because the algorithm uses the data set projected into a lower subspace, with fewer attributes needed. Also the accuracy is improved, because the label information is used for constructing a proper metric that selects the relevant attributes. However, NCA introduces a consistent training time, because we now have an objective function whose gradient is quadratic in the number of data points that is optimized using iterative methods. 

The main aim of this project is to see whether NCA's training and testing time can be reduced without significant loses in accuracy. We will try to achieve this by making use of $k$-d trees (a space partitioning structure; \citealt{bentley1975}). These have been successfully applied for speeding up $k$NN at query time \citep{friedman1977} and we will use them in a similar manner for NCA's testing operations. For training, we will apply $k$-d trees in a slightly different way (as in \citealp{deng1995}); as $k$-d trees organize the space, they can be used to group together near points and quickly compute approximations of the gradient and objective function at each iteration. This idea was also followed by \citet{weinberger2009} in accelerating a different distance metric technique, Large Margin Nearest Neighbour (LMNN). As an alternative approach, we could interpret NCA as a class-conditional kernel-density estimate and then use different types of kernels instead of the Gaussian ones; we will focus especially on kernels with compact support, because they do not introduce approximation errors and computations can be done more efficiently than in the previous case. If time permits, it would be interesting to experiment with different tree structures, such as ball trees \citep{omohundro1989, moore2000} (which can perform well in higher dimensional spaces) or dual-trees \citep{gray2003} (a faster representation of the typcal $k$-d tree). Also we could try this approach on other distance metric learning methods, with different objective functions (e.g., \citealp{xing2003}).