\chapter{Conclusions}
\label{ch:conclusions}
 
In chapter~\ref{ch:introduction} we motivated the need for metric learning in the general context of $k$NN, while section~\ref{sec:theoretical-background} advocated for metric learning using more exact mathematical arguments. We showed the equivalence between Mahalanobis metrics and linear transformation which we used for the rest of the thesis. Section~\ref{sec:related-methods} presented some of the related work in Mahalanobis metric learning.

The focus of this thesis is the NCA method. Chapter~\ref{ch:nca} explores NCA in detail and presents our experience with the method. The theoretical aspect is reviewed in section~\ref{sec:general-presentation} and we also discuss the complexity issues in more detail than they were previously treated. Our main contributions start from section~\ref{sec:cc-kde} which shows a novel interpretation of NCA\@. The class-conditional kernel density estimation, offers certain advantages (like the possibility of integrating class distribution into the model) and opens a new range of methods. In section~\ref{sec:practical-notes} we give practical advice on applying NCA.  Initializing the optimization process with a cheap linear transformation such as LDA or RCA usually results in good final solutions. For the optimization technique there does not seem to be a generally valid answer. Using either regularization or early stopping in conjunction with a NCA-based classification function can boost further the performance. Dimensionality annealing seems promising but we feel we did not experiment sufficient in this direction.

Chapter~\ref{ch:reducing} investigates methods for reducing the computational cost. The classical mini-batches method is modified to work better in a low-rank metric scenario. To do this we used cheap ways of clustering the points in low dimensions and then applied NCA on successive clusters (section~\ref{sec:mini-batches}). We evaluated these methods in an empirical setting (chapter~\ref{ch:evaluation}) and 

\section{Further directions}
\label{sec:further-directions}

This projected involved many possible directions. Unfortunately, in the short time allocated we could only look at a part of the whole picture. We aim to further some of the work in the near future. 

If time permits, it would be interesting to experiment with different tree structures, such as ball trees \citep{omohundro1989, moore2000} (which can perform well in higher dimensional spaces) or dual-trees \citep{gray2003} (a faster representation of the typcal $k$-d tree). Also we could try this approach on other distance metric learning methods, with different objective functions (e.g., \citealp{xing2003}).
