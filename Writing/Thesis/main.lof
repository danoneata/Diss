\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces The need for a metric}}{7}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Face recognition}}}{7}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Expression recogntion}}}{7}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Illustration of the equivalence between a Mahalanobis metric in the original data space and an Euclidean metric in the transformed data space. The metrics are denoted by an ellipse and, respectively, a circle whose contours indicate equal distance from the centre point to the rest of the points.}}{8}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Mahalanobis metric in the original space}}}{8}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Euclidean metric in the projected space}}}{8}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces NCA as a class-conditional kernel density estimation model. The figure represents a schematic illustration of a mixture of Gaussians. The red circles denote isotropic Gaussians. These are centred onto the points that belong to a certain class. A point\nobreakspace {}$i$ is generated by either of the components of the class with a probability inverse proportional with the distance.}}{14}
\contentsline {figure}{\numberline {3.2}{\ignorespaces This figure illustrates the early stopping procedure. During training we keep a small part of the data set apart, for cross-validation. On this cross-validation data set we compute the error function at each iteration. When the error function stops decreasing we stop the learning procedure and return to the parameter that achieved the best value. This figure resulted while applying NCA on the \texttt {mnist} data set. The best value was obtained at iteration 98, as indicated by the red dot.}}{19}
\contentsline {figure}{\numberline {3.3}{\ignorespaces \relax \fontsize {10.95}{13.6}\selectfont \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Results for different initializations (random, PCA and LDA) on \texttt {wine} data set. The images on the left side represent the projections of the data set using the initial $\mathbf {A}$. The images on the right side are data set projection with the final $\mathbf {A}$. Above each figure there is presented the LOOCV score which is normalized to 100.}}{21}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Initial random projection}}}{21}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {NCA projection after random initialization}}}{21}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {Initial projection using PCA}}}{21}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {NCA projection after PCA initialization}}}{21}
\contentsline {subfigure}{\numberline {(e)}{\ignorespaces {Initial projection using LDA}}}{21}
\contentsline {subfigure}{\numberline {(f)}{\ignorespaces {NCA projection after LDA initialization}}}{21}
\contentsline {figure}{\numberline {3.4}{\ignorespaces Example of applying the dimensionality annealing procedure to \texttt {ionosphere} data set. The method using dimensionality annealing is more visually appealing than the classical approach.}}{26}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Simple NCA transformation}}}{26}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {NCA with dimensionality annealing projection}}}{26}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Result of sub-sampling method on \texttt {wine}. There were used one third of the original data set for training, \textit {i.e.}, $n = N/3$. We note that the points that belong to the sub-set $\mathcal {D}_n$ are perfectly separated. But after applying the metric to the whole data there appear different misclassification errors. The effects are even more acute if we use smaller sub-sets.}}{28}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Learnt projection $\mathbf {A}$ on the sub-sampled data set $\mathcal {D}_n$.}}}{28}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {The projection $\mathbf {A}$ applied to the whole data set $\mathcal {D}$.}}}{28}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Evolution of the stochastic assignments $p_{ij}$ during training for a given point $\mathbf {x}_i$.}}{32}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Illustration of the $k$-d tree with bounding boxes at different levels of depths. This figure also outlines the building phases of the tree.}}{34}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Root node}}}{34}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {First level}}}{34}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {Second level}}}{34}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {Last level}}}{34}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Neighbourhood component analysis with compact support kernels and background distribution. The main assumption is that each class is a mixture of compact support distributions $k(\mathbf {x}|\mathbf {x}_j)$ plus a normal background distribution $\mathcal {N}(\mathbf {x}|\unhbox \voidb@x \hbox {\relax \mathversion {bold}$\mu $},\unhbox \voidb@x \hbox {\relax \mathversion {bold}$\Sigma $})$.}}{41}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Evolution of test accuracy as dimensionality increases on \texttt {landsat} data set. We see that NCA operates almost as well in low dimensions ($d=6,\cdots ,10$) as in high dimensions ($d>25$). This approach can be used for selecting a suitable dimension to which we project the data.}}{48}
\contentsline {figure}{\numberline {5.2}{\ignorespaces \relax \fontsize {10.95}{13.6}\selectfont \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Time \textsl {vs.} accuracy plots on larger data sets for four of the proposed methods. For SS we plotted the $1$-NN score, while for the other three the points inidicate the NCA score.}}{52}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {\texttt {usps}}}}{52}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {\texttt {magic}}}}{52}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {\texttt {mnist}}}}{52}
\contentsline {figure}{\numberline {5.3}{\ignorespaces Two dimensional projections of \texttt {usps} data set using two variants of NCA learning. The linear transformation was learnt on a training set, and here is plotted the projection of a testing set.}}{53}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {NCA MB}}}{53}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {NCA SL}}}{53}
\contentsline {figure}{\numberline {5.4}{\ignorespaces Two dimensional projections of \texttt {magic} data set using two variants of NCA learning. The linear transformation was learnt on a training set, and here is plotted the projection of a testing set.}}{53}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {NCA MB}}}{53}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {NCA SL}}}{53}
\contentsline {figure}{\numberline {5.5}{\ignorespaces This figure illustrates how the fraction of the points inspected varies during the learning procedure. When we use random initialization, there are inspected only $0.1\%$ of the points. If RCA is used to initialize the learning algorithm a fraction of about $10\%$ is used.}}{55}
\contentsline {figure}{\numberline {5.6}{\ignorespaces Illustration of final projections using two different initializations for NCA SL CS.}}{55}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {NCA SL CS randomly initialized}}}{55}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {NCA SL CS with RCA initialization}}}{55}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {B.1}{\ignorespaces \relax \fontsize {10.95}{13.6}\selectfont \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Results on \texttt {iris} data set.}}{62}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {NCA projection after random initialization}}}{62}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {NCA projection after PCA initialization}}}{62}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {NCA projection after LDA initialization}}}{62}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {NCA projection after RCA initialization}}}{62}
\contentsline {figure}{\numberline {B.2}{\ignorespaces \relax \fontsize {10.95}{13.6}\selectfont \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Results on \texttt {balance} data set.}}{63}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {NCA projection after random initialization}}}{63}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {NCA projection after PCA initialization}}}{63}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {NCA projection after LDA initialization}}}{63}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {NCA projection after RCA initialization}}}{63}
\contentsline {figure}{\numberline {B.3}{\ignorespaces \relax \fontsize {10.95}{13.6}\selectfont \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Results on \texttt {ecoli} data set.}}{64}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {NCA projection after random initialization}}}{64}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {NCA projection after PCA initialization}}}{64}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {NCA projection after LDA initialization}}}{64}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {NCA projection after RCA initialization}}}{64}
