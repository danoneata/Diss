\chapter{Introduction} {
	  

	  $k$ Nearest Neighbours ($k$NN) is one of the oldest and simplest classification methods. The underlying idea is intuitive and straightforward: classify a given point according to a majority vote of its neighbours; the selected class is the one that is the most represented amongst the $k$ nearest neighbours. This is easy to implement and it usually represents a good way to approach new problems or data sets. Despite its simplicty $k$NN is still a powerful tool, performing surprisingly well in practice, as most of the simple classifiers \cite{holte1993}.

	  However, there are also other characteristics that make $k$NN an interesting method. First of all, $k$NN makes no assumptions about the underlying structure of the data, but lets the data ``speak for themselves''. So, the accuracy increases with the number of points in the data set. In fact, it approaches Bayes optimality as the cardinality of the training set approaches infinity and $k$ is sufficiently large. Secondly, $k$NN constructs only simple local approximations; by combining all these it can represent complex global functions, which have non-linear decision boundaries and which are hardly captured parametric methdods. Laslty, $k$NN operates in a ``lazy'' fashion: the training data are just stored and their use is delayed until the the testing. The quasi-inexistent training\footnote{There can be a training step in which the parameter $k$ is tuned via cross-validation or we can we imagine different technique being applied for dimensionality reduciton} permits to easily add new training examples. On the other hand the storage is expensive and also the computational cost at testing time is often prohibitive (naively, there are $n^2$ computations to be done for each different query point). However, the drawback that affects the accuracy is how ``nearest'' is defined and how this distance is computed. 
	  
	   Neighbourhood Component Analysis (NCA) \cite{goldberger2004} is a version of $k$NN that tries to solve two of its drawbacks by learning a low-rank metric. This improves the accuracy---because the label information is now used for constructing a proper metric---and minimizes the computational cost for testing. However it introduces a training time---we have an objective function that is quadratic in the number of attributes and it has to be optimized using iterative methods.
	  
	  
		\begin{itemize}
			\item $k$-d trees \cite{bentley1975} were extensively used for accelerating range searches and nearest neighbour searches \cite{moore1991}.
			\item The main aim of the project\marginpar{Purpose} is to see if $k$-d trees can be used for reducing NCA's training and testing time without losing accuracy in the classification task. If time permits, we might investigate the use of other variants of tree structure, such as ball trees \cite{omohundro1989, omohundro1991}  (which perform better in high dimensions) or dual trees \cite{gray2003} (which scale linearly with the size of the data set). A further idea could be the use of kernels with compact support instead of the Gaussian ones in order to avoid the approximation errors introduced by $k$-d trees.
		\end{itemize}